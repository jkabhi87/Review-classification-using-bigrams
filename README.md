# Review-classification-using-bigrams
Using bigrams for sentiment analysis

Approach:
At first, I would do the stemming and stopping of both training and test data sets. After this, I iterated through the training data file to create a dictionary instance which holds the sentiment of the reviews in the training data set. The dictionary instance is named linenum_sentiment and line numbers act as keys to this dictionary while the +1 or -1 sentiment associated with review act as the value set.
Once the linenum_sentiment dictionary is loaded with values, I go on to cre- ate a dictionary element called bigram_table with the bigrams (from the re- views in training data set, i create bigrams by grouping the consecutive words in a line) acting as keys ,while the set of line numbers act as values (please note it is a set not list. This is to have a single entry for every line number or review. One review might have more than one instance of a bigram but we record the line number only once to avoid duplicate entries).
So at the end of this step, i would have created a dictionary with bigrams and the corresponding line numbers where the bigrams appear in the training data set. The creation of bigram is basically pairing up of all the words in a review from the stemmed data, iteratively.
The next step is determining the k-closest reviews in the training set for every review in the test data. For this, I iterate through each line in the test data set and generate the list of bigrams for the line. Then for each bigram, I fetch the list of line numbers associated with each bigram in that line (from the test set) from the corresponding entry in the bigram table and append the list to get the line numbers for all the bigrams in the review. This defines the feature vector for my implementation. This feature vector basically contains the list of all the line numbers of all the vectors that form the review. Once i have this vector, I search for 5 most repeated line numbers in the list of line num-
      
bers generated for the line in test data set. These 5 lines represent the 5 clos- est neighbors for the review in question. If the line number is most repeated, it would mean that this line number in the training set has the most number of matching bigrams with the test set review which determines itâ€™s similarity to the review in test data set. So, more common bigrams would correspond to more repeated line numbers.
Once i have the 5 nearest neighbors, I just lookup the sentiment correspond- ing to these nearest neighbors and the based on the number of +1s and -1s in the sentiment list, I assign a +1 or a -1 for the corresponding review in the test data set. Another thing with the computing of plus count and minus count in sentiment analysis of the review is, I assign a weight of 1 for every posi- tive review while I assign a weight of 1.15 for every negative review. This is because of the difference in the number of positive and negative reviews in the training data set. So to normalize the positive bias, I assign the weights in proportion to the training data set. I repeat this procedure for all the lines in the test data set.
Once I run the program, I redirect all the output (the +1s and the -1s) to a file and determine the working.
The reason i chose to implement bigram instead of bag of words is because, I felt that when we process the words in a sentence in groups, the probability of finding similar review in the training set increases in comparison. Also, in the end, the reviews with most of the bigram matches end up being the neighbor. This in-turn helps in choosing the reviews in the training set that are closer or similar to the reviews in the test set.
Also, I ran the code on the training set to find 1 and 3 nearest neighbors but the results were far too deviated from the training set data. 5 nearest neigh- bors gave the predictions that were closest to the training set. 7 nearest neighbors started deviating a little more than the 5 neighbor values. Hence, chose to stick with 5 nearest neighbors. Also, note that the order of running programs is as follows. First the data stopper.py, then datastemmer.py and fi- nally bigrammer_knn.py.
